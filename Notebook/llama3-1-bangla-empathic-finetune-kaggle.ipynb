{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6104837,"sourceType":"datasetVersion","datasetId":3497143}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install required packages for Kaggle environment\n!pip install -q packaging transformers datasets evaluate accelerate bitsandbytes rouge_score sacrebleu\n!pip install -U peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:28:45.446814Z","iopub.execute_input":"2026-01-06T12:28:45.447828Z","iopub.status.idle":"2026-01-06T12:28:59.266537Z","shell.execute_reply.started":"2026-01-06T12:28:45.447798Z","shell.execute_reply":"2026-01-06T12:28:59.265634Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\nCollecting peft\n  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from peft) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from peft) (25.0)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from peft) (6.0.3)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.8.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from peft) (4.57.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from peft) (4.67.1)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from peft) (0.6.2)\nRequirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from peft) (0.36.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.10.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.5)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.25.0->peft) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.4.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->peft) (0.22.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.6.2)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.11.12)\nDownloading peft-0.18.0-py3-none-any.whl (556 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.17.1\n    Uninstalling peft-0.17.1:\n      Successfully uninstalled peft-0.17.1\nSuccessfully installed peft-0.18.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install Unsloth-compatible TRL version\n!pip install -q \"trl<0.12.0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:28:59.268065Z","iopub.execute_input":"2026-01-06T12:28:59.268446Z","iopub.status.idle":"2026-01-06T12:29:03.518975Z","shell.execute_reply.started":"2026-01-06T12:28:59.268417Z","shell.execute_reply":"2026-01-06T12:29:03.518280Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install Unsloth\n!pip install unsloth -U","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:29:03.520282Z","iopub.execute_input":"2026-01-06T12:29:03.520643Z","iopub.status.idle":"2026-01-06T12:31:40.900955Z","shell.execute_reply.started":"2026-01-06T12:29:03.520615Z","shell.execute_reply":"2026-01-06T12:31:40.900218Z"}},"outputs":[{"name":"stdout","text":"Collecting unsloth\n  Downloading unsloth-2026.1.2-py3-none-any.whl.metadata (66 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting unsloth_zoo>=2026.1.2 (from unsloth)\n  Downloading unsloth_zoo-2026.1.2-py3-none-any.whl.metadata (32 kB)\nRequirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.45.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from unsloth) (25.0)\nRequirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.8.0+cu126)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.23.0+cu126)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unsloth) (2.0.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.9.5)\nRequirement already satisfied: tyro in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.0.3)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from unsloth) (5.29.5)\nCollecting xformers>=0.0.27.post2 (from unsloth)\n  Downloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.2 kB)\nRequirement already satisfied: bitsandbytes!=0.46.0,!=0.48.0,>=0.45.5 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.49.0)\nRequirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (3.4.0)\nRequirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.2.1)\nCollecting datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1 (from unsloth)\n  Downloading datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (1.11.0)\nRequirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.18.0)\nRequirement already satisfied: huggingface_hub>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.36.0)\nRequirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.1.9)\nRequirement already satisfied: diffusers in /usr/local/lib/python3.12/dist-packages (from unsloth) (0.35.2)\nRequirement already satisfied: transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3 in /usr/local/lib/python3.12/dist-packages (from unsloth) (4.57.1)\nCollecting trl!=0.19.0,<=0.24.0,>=0.18.2 (from unsloth)\n  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.3)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.34.1->unsloth) (0.6.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.20.1)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (22.0.0)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.32.5)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.6.0)\nCollecting multiprocess<0.70.17 (from datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2025.9.0,>=2023.1.0 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth)\n  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.34.0->unsloth) (1.2.1rc0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.4.0->unsloth) (1.11.1.6)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers!=4.52.0,!=4.52.1,!=4.52.2,!=4.52.3,!=4.53.0,!=4.54.0,!=4.55.0,!=4.55.1,!=4.57.0,<=4.57.3,>=4.51.3->unsloth) (0.22.1)\nCollecting torchao>=0.13.0 (from unsloth_zoo>=2026.1.2->unsloth)\n  Downloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\nCollecting cut_cross_entropy (from unsloth_zoo>=2026.1.2->unsloth)\n  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from unsloth_zoo>=2026.1.2->unsloth) (11.3.0)\nCollecting msgspec (from unsloth_zoo>=2026.1.2->unsloth)\n  Downloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\nCollecting torch>=2.4.0 (from unsloth)\n  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\nCollecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.10.2.21->torch>=2.4.0->unsloth)\n  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufft-cu12==11.3.3.83 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-curand-cu12==10.3.9.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.7.1.2->torch>=2.4.0->unsloth)\n  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nccl-cu12==2.27.5 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\nCollecting nvidia-nvshmem-cu12==3.3.20 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\nCollecting nvidia-nvtx-cu12==12.8.90 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cufft-cu12==11.3.0.4->torch>=2.4.0->unsloth)\n  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-cufile-cu12==1.13.1.3 (from torch>=2.4.0->unsloth)\n  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\nCollecting triton>=3.0.0 (from unsloth)\n  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: importlib_metadata in /usr/local/lib/python3.12/dist-packages (from diffusers->unsloth) (8.7.0)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision (from unsloth)\n  Downloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (5.9 kB)\nRequirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (0.17.0)\nRequirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from tyro->unsloth) (4.4.4)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.13.2)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (4.12.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.11.12)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.16.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.4.0->unsloth) (1.3.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata->diffusers->unsloth) (3.23.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2025.3)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets!=4.0.*,!=4.1.0,<4.4.0,>=3.4.1->unsloth) (1.17.0)\nDownloading unsloth-2026.1.2-py3-none-any.whl (381 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m381.1/381.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-4.3.0-py3-none-any.whl (506 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.24.0-py3-none-any.whl (423 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m423.1/423.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading unsloth_zoo-2026.1.2-py3-none-any.whl (295 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m295.7/295.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xformers-0.0.33.post2-cp39-abi3-manylinux_2_28_x86_64.whl (122.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m122.9/122.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.24.1-cp312-cp312-manylinux_2_28_x86_64.whl (8.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.7/146.7 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchao-0.15.0-cp310-abi3-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (7.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m120.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\nDownloading msgspec-0.20.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (224 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torchao, triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multiprocess, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch, datasets, xformers, torchvision, cut_cross_entropy, trl, unsloth_zoo, unsloth\n  Attempting uninstall: torchao\n    Found existing installation: torchao 0.10.0\n    Uninstalling torchao-0.10.0:\n      Successfully uninstalled torchao-0.10.0\n  Attempting uninstall: triton\n    Found existing installation: triton 3.4.0\n    Uninstalling triton-3.4.0:\n      Successfully uninstalled triton-3.4.0\n  Attempting uninstall: nvidia-nvtx-cu12\n    Found existing installation: nvidia-nvtx-cu12 12.6.77\n    Uninstalling nvidia-nvtx-cu12-12.6.77:\n      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n  Attempting uninstall: nvidia-nvshmem-cu12\n    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.27.3\n    Uninstalling nvidia-nccl-cu12-2.27.3:\n      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufile-cu12\n    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.18\n    Uninstalling multiprocess-0.70.18:\n      Successfully uninstalled multiprocess-0.70.18\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.10.0\n    Uninstalling fsspec-2025.10.0:\n      Successfully uninstalled fsspec-2025.10.0\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.8.0+cu126\n    Uninstalling torch-2.8.0+cu126:\n      Successfully uninstalled torch-2.8.0+cu126\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.4.1\n    Uninstalling datasets-4.4.1:\n      Successfully uninstalled datasets-4.4.1\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.23.0+cu126\n    Uninstalling torchvision-0.23.0+cu126:\n      Successfully uninstalled torchvision-0.23.0+cu126\n  Attempting uninstall: trl\n    Found existing installation: trl 0.11.4\n    Uninstalling trl-0.11.4:\n      Successfully uninstalled trl-0.11.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\ntorchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed cut_cross_entropy-25.1.1 datasets-4.3.0 fsspec-2025.9.0 msgspec-0.20.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 torchao-0.15.0 torchvision-0.24.1 triton-3.5.1 trl-0.24.0 unsloth-2026.1.2 unsloth_zoo-2026.1.2 xformers-0.0.33.post2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom datetime import datetime\nfrom abc import ABC, abstractmethod\nfrom datasets import load_dataset, Dataset\nfrom unsloth import FastLanguageModel  # MUST import Unsloth FIRST!\nfrom transformers import (\n    AutoModelForCausalLM, \n    AutoTokenizer, \n    BitsAndBytesConfig, \n    TrainingArguments\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer, SFTConfig\nimport evaluate\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:31:40.903202Z","iopub.execute_input":"2026-01-06T12:31:40.903778Z","iopub.status.idle":"2026-01-06T12:32:16.088337Z","shell.execute_reply.started":"2026-01-06T12:31:40.903748Z","shell.execute_reply":"2026-01-06T12:32:16.087409Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2026-01-06 12:31:49.410659: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767702709.580828      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767702709.630637      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1767702710.030948      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767702710.030984      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767702710.030987      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1767702710.030990      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# 1. STRATEGY INTERFACE\nclass FineTuningStrategy(ABC):\n    @abstractmethod\n    def setup_model(self, model_id, max_seq_length):\n        \"\"\"Initializes model and tokenizer based on the chosen strategy.\"\"\"\n        pass\n\n# 2.1. CONCRETE LORA STRATEGY (Standard PEFT)\nclass LoRAStrategy(FineTuningStrategy):\n    def setup_model(self, model_id, max_seq_length):\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n            bnb_4bit_use_double_quant=True,\n        )\n        \n        tokenizer = AutoTokenizer.from_pretrained(model_id)\n        # Standard fix for padding\n        tokenizer.pad_token = tokenizer.eos_token\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            model_id,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            dtype=torch.float16, \n            trust_remote_code=True\n        )\n        \n        model = prepare_model_for_kbit_training(model)\n        \n        # Optimized for Llama architectures by including MLP layers\n        lora_config = LoraConfig(\n            r=16, \n            lora_alpha=32,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                            \"gate_proj\", \"up_proj\", \"down_proj\"],\n            lora_dropout=0.05, \n            bias=\"none\", \n            task_type=\"CAUSAL_LM\"\n        )\n        model = get_peft_model(model, lora_config)\n        return model, tokenizer, lora_config\n\n# 2.2. CONCRETE Unsloth STRATEGY (Highly Optimized)\nclass UnslothStrategy(FineTuningStrategy):\n    def setup_model(self, model_id, max_seq_length):\n        # 1. Load Model and Tokenizer via Unsloth\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            model_name = model_id,\n            max_seq_length = max_seq_length,\n            dtype = None, # Auto-detects (Bfloat16 for Ampere+, Float16 for T4)\n            load_in_4bit = True,\n        )\n\n        # 2. Apply Unsloth-optimized LoRA\n        model = FastLanguageModel.get_peft_model(\n            model,\n            r = 16, \n            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n                              \"gate_proj\", \"up_proj\", \"down_proj\"],\n            lora_alpha = 32,\n            lora_dropout = 0, # Unsloth optimization: 0 is faster\n            bias = \"none\",    \n            use_gradient_checkpointing = \"unsloth\", \n            random_state = 3407,\n        )\n        \n        # 3. FIX TOKENIZER (CRITICAL for Llama 3.1)\n        # Llama 3.1 uses <|end_of_text|> but often needs <|eot_id|> for Instruct.\n        # We ensure the tokenizer recognizes the end of the string to prevent loops.\n        EOS_TOKEN = tokenizer.eos_token \n        tokenizer.pad_token = tokenizer.eos_token\n        \n        # Force model config to match tokenizer for SFTTrainer compatibility\n        if hasattr(model, 'config'):\n            model.config.pad_token_id = tokenizer.pad_token_id\n            model.config.eos_token_id = tokenizer.eos_token_id\n            \n        return model, tokenizer, None","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:16.089425Z","iopub.execute_input":"2026-01-06T12:32:16.090453Z","iopub.status.idle":"2026-01-06T12:32:16.099134Z","shell.execute_reply.started":"2026-01-06T12:32:16.090416Z","shell.execute_reply":"2026-01-06T12:32:16.098429Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\n\n# 3. DATASET PROCESSOR (Final Verified Version)\nclass DatasetProcessor:\n    def __init__(self, file_path, tokenizer):\n        self.file_path = file_path\n        self.tokenizer = tokenizer\n        self.max_seq_limit = 2048\n\n    def format_prompt(self, row):\n        # We ensure the Llama 3 special tokens are used correctly\n        # The prompt ends with the answer for SFT training\n        text = (\n            \"<|begin_of_text|>\"\n            \"<|start_header_id|>system<|end_header_id|>\\n\\n\"\n            \"à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦¸à¦¹à¦¾à¦¨à§à¦­à§‚à¦¤à¦¿à¦¶à§€à¦² à¦¬à¦¾à¦™à¦¾à¦²à¦¿ à¦®à¦¾à¦¨à¦¸à¦¿à¦• à¦¸à§à¦¬à¦¾à¦¸à§à¦¥à§à¦¯ à¦ªà¦°à¦¾à¦®à¦°à§à¦¶à¦¦à¦¾à¦¤à¦¾à¥¤<|eot_id|>\"\n            \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n            f\"à¦¬à¦¿à¦·à¦¯à¦¼: {row['Topics']} | {row['Question-Title']}\\n\"\n            f\"{row['Questions']}<|eot_id|>\"\n            \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n            f\"{row['Answers']}<|eot_id|>\"\n        )\n        return {\"text\": text}\n\n    def process_and_get_max_len(self):\n        df = pd.read_csv(self.file_path)\n        # Ensure we only process if columns exist\n        required_cols = ['Topics', 'Question-Title', 'Questions', 'Answers']\n        if not all(col in df.columns for col in required_cols):\n            raise ValueError(f\"CSV must contain: {required_cols}\")\n\n        sample_size = min(20000, len(df)) # Keep at 200 for your current test\n        df_sampled = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n        raw_ds = Dataset.from_pandas(df_sampled)\n\n        def filter_and_calculate(example):\n            formatted = self.format_prompt(example)\n            tokens = self.tokenizer.encode(formatted[\"text\"], add_special_tokens=False)\n            ans_tokens = self.tokenizer.encode(str(example['Answers']), add_special_tokens=False)\n            return {\n                \"text\": formatted[\"text\"],\n                \"total_len\": len(tokens),\n                \"answer_len\": len(ans_tokens)\n            }\n\n        processed_ds = raw_ds.map(filter_and_calculate)\n        valid_ds = processed_ds.filter(lambda x: x[\"total_len\"] <= self.max_seq_limit)\n        \n        max_answer_len = max(valid_ds[\"answer_len\"]) if len(valid_ds) > 0 else 128\n        \n        # Split Logic (90/9/1 for small sample)\n        train_test = valid_ds.train_test_split(test_size=0.10, seed=42)\n        test_val = train_test['test'].train_test_split(test_size=0.1, seed=42)\n\n        return train_test['train'], test_val['train'], test_val['test'], self.max_seq_limit, max_answer_len","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:16.100272Z","iopub.execute_input":"2026-01-06T12:32:16.100695Z","iopub.status.idle":"2026-01-06T12:32:16.126996Z","shell.execute_reply.started":"2026-01-06T12:32:16.100626Z","shell.execute_reply":"2026-01-06T12:32:16.126175Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n# 4. EXPERIMENT LOGGER (As per Requirements 2.1)\nclass ExperimentLogger:\n    \"\"\"\n    Logs experiment configurations, training metrics, and evaluation results.\n    Ensures that results like Bleu, RougeL, and Perplexity are stored for analysis.\n    \"\"\"\n    def log_to_csv(self, table_name, data):\n        \"\"\"\n        Appends a single row of data to a CSV file.\n        \n        Args:\n            table_name (str): The name of the log (e.g., 'training_results', 'eval_metrics').\n            data (dict): Dictionary containing the metrics and parameters to log.\n        \"\"\"\n        filename = f\"{table_name}.csv\"\n        \n        # Convert dictionary to DataFrame\n        df = pd.DataFrame([data])\n        \n        # Check if file exists to determine if header is needed\n        file_exists = os.path.isfile(filename)\n        \n        try:\n            # Mode 'a' for append\n            df.to_csv(filename, mode='a', header=not file_exists, index=False)\n        except Exception as e:\n            print(f\"âš ï¸ Error logging to {filename}: {e}\")\n\n    def log_eval_metrics(self, model_id, metrics):\n        \"\"\"\n        Helper specifically for the required metrics: Bleu, RougeL, Perplexity.\n        \"\"\"\n        log_data = {\n            \"model_id\": model_id,\n            \"bleu\": metrics.get(\"bleu\", 0),\n            \"rougeL\": metrics.get(\"rougeL\", 0),\n            \"perplexity\": metrics.get(\"perplexity\", 0),\n            \"timestamp\": pd.Timestamp.now()\n        }\n        self.log_to_csv(\"evaluation_history\", log_data)\n        \n    def log_experiment_summary(self, experiment_id, model_id, lora_config, train_loss, val_loss, metrics, training_time=None):\n        \"\"\"\n        Logs overall experiment details to LLAMAExperiments.csv\n        \"\"\"\n        log_data = {\n            \"id\": experiment_id,\n            \"model_name\": model_id,\n            \"lora_config\": str(lora_config) if lora_config is not None else \"Unsloth-optimized (r=16, alpha=32)\",\n            \"train_loss\": round(train_loss, 4) if train_loss is not None else None,\n            \"val_loss\": round(val_loss, 4) if val_loss is not None else None,   # â† Fixed here\n            \"perplexity\": metrics.get(\"perplexity\", None),\n            \"bleu\": metrics.get(\"bleu\", None),\n            \"rouge_l\": metrics.get(\"rouge_l\", None),\n            \"training_time_seconds\": round(training_time, 2) if training_time is not None else None,\n            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        }\n        self.log_to_csv(\"LLAMAExperiments\", log_data)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:16.128008Z","iopub.execute_input":"2026-01-06T12:32:16.128334Z","iopub.status.idle":"2026-01-06T12:32:16.141638Z","shell.execute_reply.started":"2026-01-06T12:32:16.128294Z","shell.execute_reply":"2026-01-06T12:32:16.140838Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport evaluate\nimport pandas as pd\nimport re\nfrom datetime import datetime\nfrom tqdm import tqdm\n\n# 5. EVALUATOR CLASS (Synced with Llama 3.1 Special Tokens)\nclass Evaluator:\n    def __init__(self, model, tokenizer, logger, max_new_tokens):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.logger = logger\n        self.max_new_tokens = max_new_tokens\n        \n        # Load metrics\n        self.bleu = evaluate.load(\"bleu\")\n        self.rouge = evaluate.load(\"rouge\")\n\n    @torch.inference_mode()\n    def calculate_metrics(self, test_dataset, experiment_id):\n        \"\"\"\n        Evaluation loop optimized for the Llama 3.1 Instruct header format.\n        \"\"\"\n        print(f\"\\nğŸ§ª Evaluating {len(test_dataset)} samples...\")\n        \n        predictions, references = [], []\n        total_loss, total_tokens = 0.0, 0\n        \n        self.model.eval()\n        \n        # Prompt boundary identifier from your Preprocessor\n        ASSISTANT_TAG = \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n        eos_token_id = self.tokenizer.eos_token_id\n\n        for idx, example in tqdm(enumerate(test_dataset), total=len(test_dataset), desc=\"Testing\"):\n            full_text = example['text']\n            \n            # Split input from expected output\n            if ASSISTANT_TAG in full_text:\n                parts = full_text.split(ASSISTANT_TAG)\n                input_text = parts[0] + ASSISTANT_TAG\n                expected = parts[1].split(\"<|end_of_text|>\")[0].strip()\n            else:\n                continue\n\n            inputs = self.tokenizer(input_text, return_tensors=\"pt\").to(self.model.device)\n            \n            # --- Inference Generation ---\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=self.max_new_tokens,\n                use_cache=True,\n                do_sample=True,\n                temperature=0.3,           # Low temp to keep Bengali response consistent\n                top_p=0.9,\n                repetition_penalty=1.1,   # Prevents loops without forcing English fallback\n                pad_token_id=eos_token_id,\n                eos_token_id=eos_token_id\n            )\n            \n            # Decode only the newly generated tokens\n            gen_tokens = outputs[0][inputs['input_ids'].shape[-1]:]\n            gen_text = self.tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n            \n            # Post-generation cleanup\n            gen_text = gen_text.split(\"<|\")[0].strip()\n            gen_text = gen_text.split(\"User:\")[0].strip()\n\n            # --- Perplexity Logic ---\n            enc_full = self.tokenizer(full_text, return_tensors=\"pt\").to(self.model.device)\n            enc_input = self.tokenizer(input_text, return_tensors=\"pt\").to(self.model.device)\n            \n            labels = enc_full[\"input_ids\"].clone()\n            labels[:, :enc_input[\"input_ids\"].shape[1]] = -100 \n            \n            loss_outputs = self.model(**enc_full, labels=labels)\n            num_valid_tokens = (labels != -100).sum().item()\n            \n            if num_valid_tokens > 0:\n                total_loss += loss_outputs.loss.item() * num_valid_tokens\n                total_tokens += num_valid_tokens\n\n            predictions.append(gen_text)\n            references.append(expected)\n            \n            # Individual result logging\n            self.logger.log_to_csv(\"GeneratedResponses\", {\n                \"id\": f\"TEST_{idx+1:04d}\", \n                \"experiment_id\": experiment_id,\n                \"input_text\": input_text.strip(),\n                \"expected\": expected,\n                \"generated\": gen_text,\n                \"status\": \"Bengali\" if any('\\u0980' <= c <= '\\u09FF' for c in gen_text) else \"English/Other\"\n            })\n\n        # --- Compute Final Metrics ---\n        print(\"\\nğŸ“Š Calculating final scores using sub-word tokenization...\")\n        \n        def llama_tokenizer_fn(text):\n            return self.tokenizer.convert_ids_to_tokens(self.tokenizer.encode(text, add_special_tokens=False))\n\n        try:\n            b_score = self.bleu.compute(predictions=predictions, references=[[r] for r in references], tokenizer=llama_tokenizer_fn)['bleu']\n        except: b_score = 0.0\n        \n        try:\n            r_score = self.rouge.compute(predictions=predictions, references=references, tokenizer=llama_tokenizer_fn)['rougeL']\n        except: r_score = 0.0\n        \n        avg_loss = total_loss / total_tokens if total_tokens > 0 else 0\n        metrics = {\n            \"perplexity\": round(torch.exp(torch.tensor(avg_loss)).item(), 4),\n            \"bleu\": round(b_score, 4),\n            \"rouge_l\": round(r_score, 4)\n        }\n        \n        self.logger.log_to_csv(\"EvaluationSummary\", {\n            \"experiment_id\": experiment_id,\n            \"perplexity\": metrics[\"perplexity\"],\n            \"bleu\": metrics[\"bleu\"],\n            \"rouge_l\": metrics[\"rouge_l\"],\n            \"timestamp\": datetime.now()\n        })\n        \n        return metrics\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:16.142573Z","iopub.execute_input":"2026-01-06T12:32:16.143095Z","iopub.status.idle":"2026-01-06T12:32:16.157982Z","shell.execute_reply.started":"2026-01-06T12:32:16.143066Z","shell.execute_reply":"2026-01-06T12:32:16.157143Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch\nfrom datetime import datetime\nfrom trl import SFTConfig, SFTTrainer\nfrom unsloth import FastLanguageModel\n\n# 6. MAIN TUNER CLASS\nclass LLAMAFineTuner:\n    \"\"\"\n    Handles fine-tuning using the Llama 3.1 Instruct format.\n    \"\"\"\n    def __init__(self, strategy, model_id, logger):\n        self.strategy = strategy\n        self.model_id = model_id\n        self.logger = logger\n        self.model = None\n        self.tokenizer = None\n\n    def run_training(self, train_dataset, val_dataset, max_seq_length):\n        \"\"\"\n        Executes SFT training with epoch-based evaluation.\n        \"\"\"\n        # 1. Setup model and tokenizer\n        self.model, self.tokenizer, _ = self.strategy.setup_model(self.model_id, max_seq_length)\n        \n        # 2. Configuration for SFT\n        training_args = SFTConfig(\n            output_dir=\"/kaggle/working/checkpoints\",\n            per_device_train_batch_size=4,\n            per_device_eval_batch_size=1,\n            gradient_accumulation_steps=2,\n            learning_rate=2e-4,\n            num_train_epochs=1,\n            fp16=not torch.cuda.is_bf16_supported(),\n            bf16=torch.cuda.is_bf16_supported(),\n            optim=\"adamw_8bit\",\n            weight_decay=0.01,\n            lr_scheduler_type=\"linear\",\n            seed=3407,\n            gradient_checkpointing=True,\n            logging_steps=5,\n            eval_strategy=\"epoch\",      # Trigger at the end of each epoch\n            save_strategy=\"no\",\n            report_to=\"none\",\n            dataset_text_field=\"text\",\n            max_seq_length=max_seq_length,\n        )\n        \n        # 3. Initialize Trainer\n        trainer = SFTTrainer(\n            model=self.model,\n            train_dataset=train_dataset,\n            eval_dataset=val_dataset,\n            processing_class=self.tokenizer,\n            args=training_args,\n        )\n\n        # 4. Train\n        print(\"\\nğŸš€ Training with Llama 3.1 Instruct Format...\")\n        start_time = datetime.now()\n        train_result = trainer.train()\n        training_time = (datetime.now() - start_time).total_seconds()\n        \n        # 5. Final Evaluation Check\n        # If eval_strategy=\"epoch\" didn't trigger (e.g., small dataset), we force it here\n        print(\"\\nğŸ“Š Finalizing validation metrics...\")\n        eval_metrics = trainer.evaluate()\n        final_val_loss = eval_metrics.get(\"eval_loss\", 0)\n        \n        # 6. Save model and tokenizer\n        print(f\"\\nğŸ’¾ Saving fine-tuned weights... (Final Val Loss: {final_val_loss:.4f})\")\n        self.model.save_pretrained(\"/kaggle/working/final_model\")\n        self.tokenizer.save_pretrained(\"/kaggle/working/final_model\")\n        \n        # 7. Cleanup\n        print(\"\\nğŸ§¹ Cleaning up VRAM...\")\n        del self.model\n        del trainer\n        torch.cuda.empty_cache()\n        \n        return train_result, final_val_loss, training_time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:16.158954Z","iopub.execute_input":"2026-01-06T12:32:16.159359Z","iopub.status.idle":"2026-01-06T12:32:16.172293Z","shell.execute_reply.started":"2026-01-06T12:32:16.159333Z","shell.execute_reply":"2026-01-06T12:32:16.171565Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# 7. AUTHENTICATION FUNCTION\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\ndef authenticate_huggingface():\n    \"\"\"Authenticate with Hugging Face using Kaggle secrets.\"\"\"\n    try:\n        user_secrets = UserSecretsClient()\n        token = user_secrets.get_secret(\"HF_TOKEN\")\n        login(token=token)\n        print(\"âœ… Logged in to Hugging Face successfully!\")\n    except Exception as e:\n        print(f\"âŒ Could not find HF_TOKEN in Kaggle Secrets: {e}\")\n        raise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:32:16.174182Z","iopub.execute_input":"2026-01-06T12:32:16.174444Z","iopub.status.idle":"2026-01-06T12:32:16.188970Z","shell.execute_reply.started":"2026-01-06T12:32:16.174420Z","shell.execute_reply":"2026-01-06T12:32:16.188345Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# --- TRAINING EXECUTION CELL ---\n# This cell handles the orchestration of the fine-tuning process.\n# It calculates dataset lengths before loading the model to save time and memory.\n\nfrom pathlib import Path\nimport json  # <--- ADD THIS LINE HERE\nimport torch\n# 1. Configuration & Authentication\nMODEL_ID = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\nDATASET_PATH = \"/kaggle/input/bengali-empathetic-conversations-corpus/BengaliEmpatheticConversationsCorpus .csv\"\nEXPERIMENT_ID = \"EXP_001\"\n\n# Authenticate Hugging Face\nauthenticate_huggingface()\n\n# 2. Optimized Length Calculation\n# We load just the tokenizer first to avoid a heavy 5GB model load during processing\nprint(\"ğŸ”„ Initializing tokenizer for dataset processing...\")\nfrom transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n\n# 3. Dataset Processing\nprint(\"ğŸ“Š Processing dataset and calculating sequence lengths...\")\nprocessor = DatasetProcessor(DATASET_PATH, tokenizer)\ntrain_ds, val_ds, test_ds, absolute_max_len, new_max_token = processor.process_and_get_max_len()\n\n# 4. Persistence for Inference Cell\n# We save the calculated lengths so the next cell (Inference) can read them \n# without needing to re-process the dataset.\nconfig_metadata = {\n    \"max_seq_length\": absolute_max_len,\n    \"max_new_tokens\": new_max_token,\n    \"model_id\": MODEL_ID,\n    \"experiment_id\": EXPERIMENT_ID\n}\n\nwith open(\"/kaggle/working/exp_config.json\", \"w\") as f:\n    json.dump(config_metadata, f)\n\n# Save test dataset for the separate Inference cell\ntest_ds.save_to_disk(\"/kaggle/working/test_dataset\")\nprint(f\"âœ… Metadata and Test Dataset saved to /kaggle/working/\")\n\n# 5. Fine-Tuning Execution\ntry:\n    # Initialize components\n    strategy = UnslothStrategy()\n    logger = ExperimentLogger()\n    tuner = LLAMAFineTuner(strategy, MODEL_ID, logger)\n    \n    # Run the training loop\n    # The tuner internally handles model loading, training, and memory cleanup\n    results, val_loss, training_time = tuner.run_training(\n        train_dataset=train_ds,\n        val_dataset=val_ds,\n        max_seq_length=absolute_max_len\n    )\n    \n    print(\"\\n\" + \"=\"*30)\n    print(\"ğŸ‰ TRAINING SUCCESSFUL\")\n    print(f\"ğŸ“ˆ Final Training Loss: {results.training_loss:.4f}\")\n    print(f\"ğŸ“ˆ Final Validation Loss: {val_loss:.4f}\")\n    print(f\"â±ï¸ Total Time: {training_time:.2f} seconds\")\n    print(\"=\"*30)\n    # Log full experiment summary\n    logger.log_experiment_summary(\n        experiment_id=EXPERIMENT_ID,\n        model_id=MODEL_ID,\n        lora_config=None,  # UnslothStrategy returns None for lora_config\n        train_loss=results.training_loss,\n        val_loss=val_loss,\n        metrics={},  # We'll update full metrics after evaluation\n        training_time=training_time\n    )\n    print(\"ğŸš€ You can now proceed to the Inference & Evaluation cell.\")\n\nexcept Exception as e:\n    print(f\"\\nâŒ Training failed: {str(e)}\")\n    # Explicit memory cleanup in case of crash\n    if 'tuner' in locals(): del tuner\n    torch.cuda.empty_cache()\n    raise\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-06T12:33:08.468819Z","iopub.execute_input":"2026-01-06T12:33:08.469107Z","iopub.status.idle":"2026-01-06T18:18:25.698187Z","shell.execute_reply.started":"2026-01-06T12:33:08.469080Z","shell.execute_reply":"2026-01-06T18:18:25.697402Z"}},"outputs":[{"name":"stdout","text":"âœ… Logged in to Hugging Face successfully!\nğŸ”„ Initializing tokenizer for dataset processing...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ca6a2eeeb94e11a95712df8a831072"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e117c856628b4ccab9c0145f95dfa99c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"093090cbcec64a56af6dd6a2fd0a033a"}},"metadata":{}},{"name":"stdout","text":"ğŸ“Š Processing dataset and calculating sequence lengths...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18cf49940464714952ff2d40ba06a04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ee1b8f927d48eb8148076a48e78c5e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/198 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b64e1f79595492ab04c192ef344f849"}},"metadata":{}},{"name":"stdout","text":"âœ… Metadata and Test Dataset saved to /kaggle/working/\n==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5f59b3c88a94f5dafc5cdd905686d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1f3692601014aa98225b6c5f7619427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd7e227add694e9e8c47a14b8a534126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d327eb1b5fcb4737a6abe493affb30e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed9b03177214db384c9fac6b30b8549"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2026.1.2 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/17805 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81f7bf0d4fad4228ba73709991fc5859"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=8):   0%|          | 0/1781 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5d2999c3f34b11a08770858d8862a0"}},"metadata":{}},{"name":"stderr","text":"The model is already on multiple devices. Skipping the move to device specified in `args`.\n","output_type":"stream"},{"name":"stdout","text":"ğŸ¦¥ Unsloth: Padding-free auto-enabled, enabling faster training.\n\nğŸš€ Training with Llama 3.1 Instruct Format...\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 17,805 | Num Epochs = 1 | Total steps = 2,226\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 2 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2226' max='2226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2226/2226 5:27:10, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.417800</td>\n      <td>0.387612</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"name":"stdout","text":"\nğŸ“Š Finalizing validation metrics...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1781' max='1781' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1781/1781 16:01]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nğŸ’¾ Saving fine-tuned weights... (Final Val Loss: 0.3876)\n\nğŸ§¹ Cleaning up VRAM...\n\n==============================\nğŸ‰ TRAINING SUCCESSFUL\nğŸ“ˆ Final Training Loss: 0.4512\nğŸ“ˆ Final Validation Loss: 0.3876\nâ±ï¸ Total Time: 19652.64 seconds\n==============================\nğŸš€ You can now proceed to the Inference & Evaluation cell.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- TESTING & INFERENCE EXECUTION ---\n# Run this cell after training is complete to evaluate the fine-tuned model.\n\nimport json\nimport torch\nfrom datetime import datetime\nfrom datasets import load_from_disk\nfrom unsloth import FastLanguageModel\n\nif __name__ == \"__main__\":\n    EXPERIMENT_ID = \"EXP_001\"\n    CONFIG_PATH = \"/kaggle/working/exp_config.json\"\n    DATASET_SAVE_PATH = \"/kaggle/working/test_dataset\"\n    MODEL_PATH = \"/kaggle/working/final_model\"\n    \n    try:\n        # 1. Load Configuration from Training\n        print(f\"ğŸ“‚ Loading experiment configuration...\")\n        with open(CONFIG_PATH, \"r\") as f:\n            config = json.load(f)\n        \n        saved_max_len = config[\"max_seq_length\"]\n        saved_new_tokens = config[\"max_new_tokens\"]\n        \n        # 2. Load the saved test dataset\n        test_ds = load_from_disk(DATASET_SAVE_PATH)\n        print(f\"âœ… Loaded {len(test_ds)} test samples\")\n\n        # 3. Load Model for Inference\n        # We load directly here to avoid the AttributeError from the Tuner class\n        print(f\"ğŸ’¾ Loading fine-tuned model from {MODEL_PATH}...\")\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            model_name=MODEL_PATH,\n            max_seq_length=saved_max_len,\n            dtype=None,\n            load_in_4bit=True,\n        )\n        FastLanguageModel.for_inference(model)\n\n        # 4. Run Evaluation using the Evaluator class\n        logger = ExperimentLogger()\n        evaluator = Evaluator(model, tokenizer, logger, max_new_tokens=saved_new_tokens)\n        \n        test_metrics = evaluator.calculate_metrics(\n            test_dataset=test_ds, \n            experiment_id=EXPERIMENT_ID\n        )\n\n        # 5. Final Logging\n        print(\"\\n\" + \"=\"*30)\n        print(\"ğŸ‰ EVALUATION COMPLETE\")\n        print(f\"ğŸ“Š Perplexity: {test_metrics['perplexity']:.4f}\")\n        print(f\"ğŸ“Š BLEU:       {test_metrics['bleu']:.4f}\")\n        print(f\"ğŸ“Š ROUGE-L:    {test_metrics['rouge_l']:.4f}\")\n        print(\"=\"*30)\n        # Update the experiment log with final evaluation metrics\n        logger.log_experiment_summary(\n            experiment_id=EXPERIMENT_ID,\n            model_id=MODEL_ID,\n            lora_config=None,\n            train_loss=None,  # Already logged in training phase\n            val_loss=None,\n            metrics=test_metrics,\n            training_time=None\n        )\n\n        # Cleanup memory\n        del model\n        torch.cuda.empty_cache()\n\n    except Exception as e:\n        print(f\"âŒ Testing failed: {str(e)}\")\n        if 'model' in locals(): del model\n        torch.cuda.empty_cache()\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T18:18:25.700029Z","iopub.execute_input":"2026-01-06T18:18:25.700663Z","iopub.status.idle":"2026-01-06T18:49:00.851333Z","shell.execute_reply.started":"2026-01-06T18:18:25.700629Z","shell.execute_reply":"2026-01-06T18:49:00.850574Z"}},"outputs":[{"name":"stdout","text":"ğŸ“‚ Loading experiment configuration...\nâœ… Loaded 198 test samples\nğŸ’¾ Loading fine-tuned model from /kaggle/working/final_model...\n==((====))==  Unsloth 2026.1.2: Fast Llama patching. Transformers: 4.57.1.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nunsloth/meta-llama-3.1-8b-instruct-bnb-4bit does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3cd83fbd81345779a66a394d59ec331"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81e36a24767e4f9abe07dfef7cd814c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e275053f58f84c4e830ea48b050d147a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d73827d16f54242a15b0386ab02d742"}},"metadata":{}},{"name":"stdout","text":"\nğŸ§ª Evaluating 198 samples...\n","output_type":"stream"},{"name":"stderr","text":"Testing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [30:02<00:00,  9.11s/it] \n","output_type":"stream"},{"name":"stdout","text":"\nğŸ“Š Calculating final scores using sub-word tokenization...\n\n==============================\nğŸ‰ EVALUATION COMPLETE\nğŸ“Š Perplexity: 1.8306\nğŸ“Š BLEU:       0.2276\nğŸ“Š ROUGE-L:    0.3598\n==============================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import shutil\nimport os\n\nzip_name = \"kaggle_working_all.zip\"\nwork_dir = \"/kaggle/working\"\n\n# Remove old zip if exists\nif os.path.exists(zip_name):\n    os.remove(zip_name)\n\n# Create zip\nshutil.make_archive(\n    base_name=zip_name.replace(\".zip\", \"\"),\n    format=\"zip\",\n    root_dir=work_dir\n)\n\nprint(f\"âœ… Created {zip_name}\")\nprint(\"ğŸ“¥ You can now download it from the right panel (Output files).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-06T18:50:04.531320Z","iopub.execute_input":"2026-01-06T18:50:04.532014Z","iopub.status.idle":"2026-01-06T18:50:09.785606Z","shell.execute_reply.started":"2026-01-06T18:50:04.531981Z","shell.execute_reply":"2026-01-06T18:50:09.784610Z"}},"outputs":[{"name":"stdout","text":"âœ… Created kaggle_working_all.zip\nğŸ“¥ You can now download it from the right panel (Output files).\n","output_type":"stream"}],"execution_count":14}]}